\prefaceEn{
The work discussed in this thesis took place during my internship at King Abdullah University of Science and Technology in Saudi Arabia. As part of a team that is active in the area of distributed, large-scale networked systems, I was involved in an ongoing research around the topic of compressed communication in distributed deep learning frameworks.

My main focus was to investigate ways of minimizing the network traffic during the distributed training of deep neural networks by building various compression techniques on top of some already existing gradient sparsification methods. 

% The greatest portion of my work is dedicated in finding ways to leverage the space-efficiency properties of the bloom filter data structures by trying to incorporate them in the above setting.

% \todo[inline]{add more content}

}